{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) Project overview (one-line)\n",
        "\n",
        "Predict daily cryptocurrency volatility using historical OHLC + volume + market-cap data to forecast periods of heightened volatility for risk management and trading decisions.\n",
        "\n",
        "2) Definition of the target (how to label volatility)\n",
        "\n",
        "Two common approaches — regression or classification. Pick one:\n",
        "\n",
        "A. Regression target (continuous)\n",
        "\n",
        "Compute daily log returns: r_t = ln(close_t / close_{t-1}).\n",
        "\n",
        "Volatility target = rolling standard deviation of returns over a window W (e.g., 7 or 21 days): vol_t = std(r_{t-W+1..t}).\n",
        "\n",
        "Model predicts continuous vol_t.\n",
        "\n",
        "B. Classification target (discrete levels)\n",
        "\n",
        "After computing rolling vol (as above), define thresholds (quantiles) to label low / medium / high volatility. Example: bottom 40% → low, middle 40% → medium, top 20% → high.\n",
        "\n",
        "Model predicts class.\n",
        "\n",
        "Recommendation: start with regression (predict vol_t) then optionally categorize predicted values for alerts.\n",
        "\n",
        "3) Data preprocessing & cleaning (detailed steps)\n",
        "\n",
        "Load & parse\n",
        "\n",
        "Parse date to datetime, sort by (symbol, date) ascending.\n",
        "\n",
        "Missing values\n",
        "\n",
        "If a whole day for a symbol is missing: consider forward/backward fill only if contiguous small gaps; otherwise drop those days for that symbol.\n",
        "\n",
        "For small gaps in price/volume: forward-fill price-related fields (or linear interpolation) but do not forward-fill volume blindly — better to set to 0 or interpolate depending on domain knowledge.\n",
        "\n",
        "Remove rows with impossible values: negative prices, negative market cap (- unless corrections).\n",
        "\n",
        "Outliers\n",
        "\n",
        "Winsorize extremely large spikes or log-transform volume/market cap (see Feature Engineering).\n",
        "\n",
        "Resampling\n",
        "\n",
        "Data is daily — keep daily. If you need uniformity across symbols, ensure any missing day rows are present (NaNs) so rolling computations align.\n",
        "\n",
        "Scaling\n",
        "\n",
        "For tree models: scaling not strictly required. For linear models / neural nets: standardize or MinMax scale per feature (fit only on training set). Use StandardScaler or RobustScaler for heavy outliers.\n",
        "\n",
        "Stationarity / leakage\n",
        "\n",
        "Ensure no future information leaks into features (use only history up to day t to predict vol_{t+1} if forecasting next day).\n",
        "\n",
        "4) Feature engineering (high-value features)\n",
        "\n",
        "Base features (from OHLC, volume, market cap):\n",
        "\n",
        "open, high, low, close, volume, market_cap\n",
        "\n",
        "log_volume = ln(volume + 1), log_mcap = ln(market_cap + 1)\n",
        "\n",
        "returns = ln(close/close.shift(1))\n",
        "\n",
        "range = high - low\n",
        "\n",
        "log_range = ln((high - low)/close + eps)\n",
        "\n",
        "Rolling features (window sizes: 3, 7, 14, 21, 30):\n",
        "\n",
        "Rolling mean of returns: ma_return_W\n",
        "\n",
        "Rolling std of returns (this is rolling volatility): roll_std_W\n",
        "\n",
        "Rolling skew/kurtosis of returns\n",
        "\n",
        "Rolling median volume, rolling max volume\n",
        "\n",
        "Liquidity ratio: volume / market_cap and rolling mean of it\n",
        "\n",
        "Technical indicators:\n",
        "\n",
        "Moving averages: SMA (7, 21) and EMA (7, 21)\n",
        "\n",
        "Bollinger Bands: band width = (upper - lower) / middle\n",
        "\n",
        "ATR (Average True Range) and ATR normalized by price (ATR / close)\n",
        "\n",
        "RSI (14-day)\n",
        "\n",
        "MACD (typical)\n",
        "\n",
        "Momentum: close - close.shift(W)\n",
        "\n",
        "VWAP (if intraday not available, skip)\n",
        "\n",
        "Cross-asset / market features:\n",
        "\n",
        "Market-wide volatility: mean rolling volatility across top N cryptocurrencies that day\n",
        "\n",
        "Dominance ratio: market_cap(symbol) / total_market_cap — when dominance shifts volatility patterns may change\n",
        "\n",
        "Calendar features:\n",
        "\n",
        "day_of_week, day_of_month, is_month_end, is_quarter_end\n",
        "\n",
        "Lagged targets:\n",
        "\n",
        "Lagged rolling vol (e.g., vol_t-1, vol_t-7)\n",
        "\n",
        "Feature notes:\n",
        "\n",
        "Compute indicators per symbol using only past data.\n",
        "\n",
        "Avoid too many highly collinear features (drop or use PCA if needed).\n",
        "\n",
        "5) Model choices (baseline → advanced)\n",
        "\n",
        "Baselines:\n",
        "\n",
        "Naïve: predict last observed rolling vol (persistence).\n",
        "\n",
        "Linear Regression / Ridge / Lasso on features.\n",
        "\n",
        "Tree-based:\n",
        "\n",
        "RandomForestRegressor\n",
        "\n",
        "XGBoost / LightGBM — typically give the best tabular performance and handle missing values.\n",
        "\n",
        "Time-series models:\n",
        "\n",
        "ARIMA / GARCH family — good for direct volatility modeling per symbol (econometrics approach). GARCH(1,1) is classic for volatility; but you have many assets — can run per symbol.\n",
        "\n",
        "Deep learning:\n",
        "\n",
        "LSTM / GRU on sequences of price & engineered features (works if plenty of data per symbol).\n",
        "\n",
        "Temporal Convolutional Networks (TCN) or Transformer-based time-series models for larger datasets.\n",
        "\n",
        "Recommendation: Start with XGBoost for cross-symbol model, then try per-symbol GARCH or LSTM ensemble if time permits.\n",
        "\n",
        "6) Cross-validation & time-series splitting\n",
        "\n",
        "Use time-series aware split (e.g., TimeSeriesSplit) or expanding window validation.\n",
        "\n",
        "If training a cross-symbol global model, ensure splitting keeps time ordering per symbol and that train/test do not leak future days.\n",
        "\n",
        "Typical splits: train up to t0, validation t0+1..t1, test t1+1..t2. Use several rolling windows for robust estimates.\n",
        "\n",
        "7) Evaluation metrics\n",
        "\n",
        "If regression:\n",
        "\n",
        "RMSE, MAE\n",
        "\n",
        "R² (but may be misleading for heteroskedastic data)\n",
        "\n",
        "MAPE is poor with values near zero — use with caution.\n",
        "\n",
        "If classification:\n",
        "\n",
        "Accuracy, Precision/Recall per class, F1 (macro), ROC-AUC (one-vs-rest), confusion matrix.\n",
        "\n",
        "Business metrics:\n",
        "\n",
        "How often model correctly flags high volatility days (recall on high class) — crucial for risk alerts.\n",
        "\n",
        "8) Hyperparameter tuning & model optimization\n",
        "\n",
        "Use Optuna or scikit-learn's RandomizedSearchCV with time-series CV. For XGBoost tune: n_estimators, max_depth, learning_rate, subsample, colsample_bytree, reg_lambda, min_child_weight.\n",
        "\n",
        "Limit hyperparameter search to reasonable ranges and use early stopping on validation set.\n",
        "\n",
        "9) Pipeline & reproducibility\n",
        "\n",
        "Use sklearn.pipeline.Pipeline for preprocessing steps (imputation, scaling, feature transforms).\n",
        "\n",
        "Save fitted transformers and models with joblib or pickle.\n",
        "\n",
        "Maintain a requirements.txt (or environment.yml) with exact package versions.\n",
        "\n",
        "Set seed for reproducibility.\n",
        "\n",
        "10) Deployment (local testing, simple UI)\n",
        "\n",
        "Options:\n",
        "\n",
        "Streamlit — fast to build an interactive demo (user selects crypto ticker + date range, shows actual vs predicted vol plot and warnings).\n",
        "\n",
        "Flask — if you want a REST API: /predict returns JSON with predicted vol for next day.\n",
        "\n",
        "Containerize with Docker for portability.\n",
        "\n",
        "Streamlit skeleton: accept ticker/date → load saved model & scaler → show input features and predicted vol + classification level.\n",
        "\n",
        "11) Deliverables checklist (what to include in repo)\n",
        "\n",
        "data/ (raw/processed sample or pointer to dataset)\n",
        "\n",
        "notebooks/EDA.ipynb (visual EDA)\n",
        "\n",
        "notebooks/modeling.ipynb (feature engineering + training)\n",
        "\n",
        "src/:\n",
        "\n",
        "data_processing.py\n",
        "\n",
        "features.py\n",
        "\n",
        "train.py\n",
        "\n",
        "predict.py\n",
        "\n",
        "models/ (saved models)\n",
        "\n",
        "streamlit_app.py or app.py (Flask)\n",
        "\n",
        "README.md (project overview + how to run)\n",
        "\n",
        "HLD.pdf and LLD.pdf (text/docs)\n",
        "\n",
        "report_final.pdf\n",
        "\n",
        "requirements.txt or environment.yml\n",
        "\n",
        "evaluation/metrics.csv and figures/ (plots)\n",
        "\n",
        "12) Example code snippets\n",
        "\n",
        "These are compact — paste into a notebook or .py file.\n",
        "\n",
        "A) Compute volatility target + basic features (pandas):"
      ],
      "metadata": {
        "id": "jk-8mKsJzGBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# df: columns ['date','symbol','open','high','low','close','volume','market_cap']\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(['symbol','date']).reset_index(drop=True)\n",
        "\n",
        "# log returns\n",
        "df['log_ret'] = np.log(df['close'] / df.groupby('symbol')['close'].shift(1))\n",
        "\n",
        "# rolling volatility (e.g., 21-day) using returns\n",
        "W = 21\n",
        "df['roll_vol_21'] = df.groupby('symbol')['log_ret'].rolling(window=W, min_periods=5).std().reset_index(level=0, drop=True)\n",
        "\n",
        "# moving averages and liquidity ratio\n",
        "df['ma7'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
        "df['vol_mcap_ratio'] = df['volume'] / (df['market_cap'] + 1)\n",
        "df['roll_vol_mcap_7'] = df.groupby('symbol')['vol_mcap_ratio'].transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
        "\n",
        "# target: predict next-day volatility (shift -1)\n",
        "df['target_vol_next'] = df.groupby('symbol')['roll_vol_21'].shift(-1)\n",
        "\n",
        "# drop rows with NaN target\n",
        "df = df.dropna(subset=['target_vol_next'])\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "xyqMnl-izIrg",
        "outputId": "17351972-cbc1-4e44-e443-0a3d5dab3521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport pandas as pd\\nimport numpy as np\\n\\n# df: columns ['date','symbol','open','high','low','close','volume','market_cap']\\ndf['date'] = pd.to_datetime(df['date'])\\ndf = df.sort_values(['symbol','date']).reset_index(drop=True)\\n\\n# log returns\\ndf['log_ret'] = np.log(df['close'] / df.groupby('symbol')['close'].shift(1))\\n\\n# rolling volatility (e.g., 21-day) using returns\\nW = 21\\ndf['roll_vol_21'] = df.groupby('symbol')['log_ret'].rolling(window=W, min_periods=5).std().reset_index(level=0, drop=True)\\n\\n# moving averages and liquidity ratio\\ndf['ma7'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(7, min_periods=1).mean())\\ndf['vol_mcap_ratio'] = df['volume'] / (df['market_cap'] + 1)\\ndf['roll_vol_mcap_7'] = df.groupby('symbol')['vol_mcap_ratio'].transform(lambda x: x.rolling(7, min_periods=1).mean())\\n\\n# target: predict next-day volatility (shift -1)\\ndf['target_vol_next'] = df.groupby('symbol')['roll_vol_21'].shift(-1)\\n\\n# drop rows with NaN target\\ndf = df.dropna(subset=['target_vol_next'])\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Time-aware train/test split:"
      ],
      "metadata": {
        "id": "z6nyKpebzPyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "# If you want multiple folds per symbol, build folds on date index\n",
        "# Example: global model, sort by date\n",
        "df_global = df.sort_values('date')\n",
        "# Create train/val/test splits by date cutoffs\n",
        "train = df_global[df_global['date'] < '2023-01-01']\n",
        "val   = df_global[(df_global['date'] >= '2023-01-01') & (df_global['date'] < '2024-01-01')]\n",
        "test  = df_global[df_global['date'] >= '2024-01-01']\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "irmBciBezRt6",
        "outputId": "75cd881d-a3a8-4432-80e5-973ebdaf9e74"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom sklearn.model_selection import TimeSeriesSplit\\n# If you want multiple folds per symbol, build folds on date index\\n# Example: global model, sort by date\\ndf_global = df.sort_values('date')\\n# Create train/val/test splits by date cutoffs\\ntrain = df_global[df_global['date'] < '2023-01-01']\\nval   = df_global[(df_global['date'] >= '2023-01-01') & (df_global['date'] < '2024-01-01')]\\ntest  = df_global[df_global['date'] >= '2024-01-01']\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Train XGBoost (regression):"
      ],
      "metadata": {
        "id": "2zglNW_OzU7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = ['ma7','vol_mcap_ratio','roll_vol_21','log_ret']  # extend this list\n",
        "X_train = train[features]\n",
        "y_train = train['target_vol_next']\n",
        "X_val = val[features]; y_val = val['target_vol_next']\n",
        "\n",
        "# scale for stability (optional for XGBoost)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s = scaler.transform(X_val)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train_s, label=y_train)\n",
        "dval = xgb.DMatrix(X_val_s, label=y_val)\n",
        "\n",
        "params = {'objective':'reg:squarederror','eval_metric':'rmse','learning_rate':0.05,'max_depth':6}\n",
        "watchlist = [(dtrain,'train'), (dval,'eval')]\n",
        "bst = xgb.train(params, dtrain, num_boost_round=2000, early_stopping_rounds=50, evals=watchlist, verbose_eval=50)\n",
        "\n",
        "# evaluate\n",
        "preds = bst.predict(xgb.DMatrix(X_val_s))\n",
        "print('RMSE', mean_squared_error(y_val, preds, squared=False))\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Ju5X2M8wzZ7E",
        "outputId": "73c19130-2593-43fc-9118-2d24408dc488"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport xgboost as xgb\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.preprocessing import StandardScaler\\n\\nfeatures = ['ma7','vol_mcap_ratio','roll_vol_21','log_ret']  # extend this list\\nX_train = train[features]\\ny_train = train['target_vol_next']\\nX_val = val[features]; y_val = val['target_vol_next']\\n\\n# scale for stability (optional for XGBoost)\\nscaler = StandardScaler()\\nX_train_s = scaler.fit_transform(X_train)\\nX_val_s = scaler.transform(X_val)\\n\\ndtrain = xgb.DMatrix(X_train_s, label=y_train)\\ndval = xgb.DMatrix(X_val_s, label=y_val)\\n\\nparams = {'objective':'reg:squarederror','eval_metric':'rmse','learning_rate':0.05,'max_depth':6}\\nwatchlist = [(dtrain,'train'), (dval,'eval')]\\nbst = xgb.train(params, dtrain, num_boost_round=2000, early_stopping_rounds=50, evals=watchlist, verbose_eval=50)\\n\\n# evaluate\\npreds = bst.predict(xgb.DMatrix(X_val_s))\\nprint('RMSE', mean_squared_error(y_val, preds, squared=False))\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Simple Streamlit app skeleton:"
      ],
      "metadata": {
        "id": "SqArmoVnzdsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "st.title(\"Crypto Volatility Predictor\")\n",
        "\n",
        "model = joblib.load('models/xgb_vol_model.joblib')\n",
        "scaler = joblib.load('models/scaler.joblib')\n",
        "\n",
        "ticker = st.selectbox(\"Ticker\", options=['BTC','ETH','ADA'])  # populate dynamically from saved list\n",
        "date = st.date_input(\"Select date\", value=pd.to_datetime('2024-01-01'))\n",
        "\n",
        "if st.button(\"Predict next-day volatility\"):\n",
        "    # load last n days features for ticker, compute features, scale, predict\n",
        "    X = ... # build feature row\n",
        "    Xs = scaler.transform([X])\n",
        "    pred = model.predict(xgb.DMatrix(Xs))\n",
        "    st.write(f\"Predicted next-day vol (21d): {pred[0]:.6f}\")\n",
        "    # plot historic vs predicted\n",
        "    st.line_chart(...)\n",
        "v\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "usY3G11izh73",
        "outputId": "2dc733e0-473d-49a9-c9e3-487b37f4b655"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# streamlit_app.py\\nimport streamlit as st\\nimport pandas as pd\\nimport joblib\\nimport matplotlib.pyplot as plt\\n\\nst.title(\"Crypto Volatility Predictor\")\\n\\nmodel = joblib.load(\\'models/xgb_vol_model.joblib\\')\\nscaler = joblib.load(\\'models/scaler.joblib\\')\\n\\nticker = st.selectbox(\"Ticker\", options=[\\'BTC\\',\\'ETH\\',\\'ADA\\'])  # populate dynamically from saved list\\ndate = st.date_input(\"Select date\", value=pd.to_datetime(\\'2024-01-01\\'))\\n\\nif st.button(\"Predict next-day volatility\"):\\n    # load last n days features for ticker, compute features, scale, predict\\n    X = ... # build feature row\\n    Xs = scaler.transform([X])\\n    pred = model.predict(xgb.DMatrix(Xs))\\n    st.write(f\"Predicted next-day vol (21d): {pred[0]:.6f}\")\\n    # plot historic vs predicted\\n    st.line_chart(...)\\nv\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) EDA: plots to include\n",
        "\n",
        "Price & volume time series per symbol.\n",
        "\n",
        "Rolling volatility over time (7/21/30).\n",
        "\n",
        "Correlation matrix of features.\n",
        "\n",
        "Distribution of target volatility (histogram + log-scale).\n",
        "\n",
        "Heatmap of correlation between crypto assets' volatilities (co-movement).\n",
        "\n",
        "Feature importance bar chart (for tree models).\n",
        "\n",
        "Confusion matrix (if classification).\n",
        "\n",
        "14) HLD / LLD quick points\n",
        "\n",
        "HLD (high-level):\n",
        "\n",
        "Ingest historical data → preprocessing → feature engineering → model training & evaluation → saved model → deployment (Streamlit/Flask).\n",
        "\n",
        "LLD (low-level):\n",
        "\n",
        "Data schemas, functions:\n",
        "\n",
        "load_data(symbols, path) -> DataFrame\n",
        "\n",
        "clean_data(df) -> df_clean\n",
        "\n",
        "compute_features(df) -> df_feat\n",
        "\n",
        "train_model(X_train, y_train, params) -> model\n",
        "\n",
        "evaluate_model(model, X_test, y_test) -> metrics\n",
        "\n",
        "serve_predict(model, scaler, input_json) -> prediction_json\n",
        "\n",
        "Pipeline diagram: show arrows for each stage, indicate artifacts saved (.joblib, scaler, feature list JSON).\n",
        "\n",
        "15) Tips & pitfalls\n",
        "\n",
        "Leaky features: never use future returns or future volatility when building features. Always shift to ensure causality.\n",
        "\n",
        "Nonstationarity: crypto markets change; re-train model regularly and monitor drift.\n",
        "\n",
        "Data quality: exchanges sometimes report spurious spikes — consider cleaning.\n",
        "\n",
        "Imbalanced classes (if classifying high-vol): use class weighting or oversampling in training.\n",
        "\n",
        "16) GitHub README template (short)\n",
        "# Crypto Volatility Prediction\n",
        "\n",
        "## Overview\n",
        "Predicts daily volatility for cryptocurrencies using OHLC, volume and market-cap.\n",
        "\n",
        "## Repo structure\n",
        "- notebooks/\n",
        "- src/\n",
        "- models/\n",
        "- data/\n",
        "- streamlit_app.py\n",
        "\n",
        "## How to run\n",
        "1. `pip install -r requirements.txt`\n",
        "2. Prepare data in `data/raw/`\n",
        "3. Run EDA: `notebooks/EDA.ipynb`\n",
        "4. Train: `python src/train.py --config config.yaml`\n",
        "5. Run demo: `streamlit run streamlit_app.py`\n",
        "\n",
        "## Deliverables\n",
        "- Trained model, EDA report, HLD/LLD, final report.\n",
        "\n",
        "17) Estimated project milestones (for planning)\n",
        "\n",
        "Week 1: Data ingestion + cleaning + EDA\n",
        "\n",
        "Week 2: Feature engineering + baseline models\n",
        "\n",
        "Week 3: Advanced models + hyperparameter tuning\n",
        "\n",
        "Week 4: Final evaluation + documentation + deployment"
      ],
      "metadata": {
        "id": "aQvLtbt-zoVq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52FoV0sgzz86"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}